{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Brief description of the problem and data (5 pts)**\n",
        "### **1.1 Issue Overview**\n",
        "This project will participate in a Kaggle competition.\n",
        "\n",
        "https://www.kaggle.com/c/nlp-getting-started/overview\n",
        "\n",
        "In this assignment I have to categorize text into two groups. Determining if a tweet is discussing a disaster or not real events are involved as part of the task, at hand in this Natural Language Processing ( NLP) project which is, about helping machines comprehend and analyze human language.\n",
        "\n",
        "\n",
        "### **1.2 Natural Language Processing (NLP)**\n",
        "Natural Language Processing (or NLP) is an area, within intelligence that centers around how computers and human language interact, with each other effectively and meaningfully. It covers activities like organizing text into categories based classification purposes or understanding the context in which certain languages are used for translation purposes. For this assignment we are working with text categorization using tweets which are divided into two groups;\n",
        "\n",
        "1: The tweet refers to a real disaster.\n",
        "0: The tweet is not related to a real disaster (it could be a casual remark, metaphor, or joke).\n",
        "\n",
        "Transformer models such, as **BERT** and **DistilBERT** are widely utilized in natural language processing tasks due, to their ability to grasp the intended meaning and context of words and sentences."
      ],
      "metadata": {
        "id": "k55ho3tqXeQH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKYHaWVFMhe2",
        "outputId": "a284e92b-44da-4b2f-fe23-31f23c999f83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data Head:\n",
            "   id keyword location                                               text  \\\n",
            "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
            "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
            "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
            "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
            "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
            "\n",
            "   target  \n",
            "0       1  \n",
            "1       1  \n",
            "2       1  \n",
            "3       1  \n",
            "4       1  \n",
            "\n",
            "Test Data Head:\n",
            "   id keyword location                                               text\n",
            "0   0     NaN      NaN                 Just happened a terrible car crash\n",
            "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
            "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
            "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
            "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n",
            "\n",
            "Sample Submission Head:\n",
            "   id  target\n",
            "0   0       0\n",
            "1   2       0\n",
            "2   3       0\n",
            "3   9       0\n",
            "4  11       0\n",
            "Train Data Size: 7613 rows, 5 columns\n",
            "Test Data Size: 3263 rows, 4 columns\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "\n",
        "# Step 1: Load the datasets\n",
        "train_file_path = 'drive/MyDrive/DTSA511/Week4/train.csv'\n",
        "test_file_path = 'drive/MyDrive/DTSA511/Week4/test.csv'\n",
        "sample_submission_file_path = 'drive/MyDrive/DTSA511/Week4/sample_submission.csv'\n",
        "\n",
        "# Reading the data files\n",
        "train_data = pd.read_csv(train_file_path)\n",
        "test_data = pd.read_csv(test_file_path)\n",
        "sample_submission_data = pd.read_csv(sample_submission_file_path)\n",
        "\n",
        "# Displaying the first few rows of each dataset to understand their structure\n",
        "train_data_head = train_data.head()\n",
        "test_data_head = test_data.head()\n",
        "sample_submission_head = sample_submission_data.head()\n",
        "\n",
        "# Displaying the first few rows of each dataset to understand their structure\n",
        "print(\"Train Data Head:\")\n",
        "print(train_data.head())\n",
        "\n",
        "print(\"\\nTest Data Head:\")\n",
        "print(test_data.head())\n",
        "\n",
        "print(\"\\nSample Submission Head:\")\n",
        "print(sample_submission_data.head())\n",
        "\n",
        "# Counting the size (rows and columns)\n",
        "train_size = train_data.shape  # (number of rows, number of columns)\n",
        "test_size = test_data.shape    # (number of rows, number of columns)\n",
        "\n",
        "# Display the sizes\n",
        "print(f\"Train Data Size: {train_size[0]} rows, {train_size[1]} columns\")\n",
        "print(f\"Test Data Size: {test_size[0]} rows, {test_size[1]} columns\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.3 Dataset Details**\n",
        "#### **1.3.1. Training Dataset (train.csv)**\n",
        "Size: 7,613 samples (rows).\n",
        "Columns:\n",
        "- id: A unique identifier for each tweet.  \n",
        "- keyword: A specific keyword extracted from the tweet (optional).  \n",
        "- location: The location associated with the tweet (optional).  \n",
        "- text: The actual content of the tweet, which is the main feature for classification.  \n",
        "- target: The label indicating whether the tweet is about a real disaster (1) or not (0).  \n",
        "\n",
        "#### **1.3.2. Test Dataset (test.csv)**\n",
        "Size: 3,263 samples.\n",
        "Columns:\n",
        "- id: A unique identifier for each tweet.\n",
        "- keyword: A keyword extracted from the tweet (optional).\n",
        "- location: The location associated with the tweet (optional).\n",
        "- text: The tweet content (used for prediction)."
      ],
      "metadata": {
        "id": "76wJ2IbJaPPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Exploratory Data Analysis (EDA) â€” Inspect, Visualize and Clean the Data (15 pts)**"
      ],
      "metadata": {
        "id": "3mfUlVPHiHiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Exploratory Data Analysis (EDA)\n",
        "\n",
        "# Checking for missing values in train and test data\n",
        "missing_train = train_data.isnull().sum()\n",
        "missing_test = test_data.isnull().sum()\n",
        "\n",
        "# Plotting the distribution of the target variable in the training set\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x='target', data=train_data)\n",
        "plt.title('Distribution of Target Variable (Disaster vs Non-Disaster)')\n",
        "plt.xlabel('Target')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Add a column for the length of each tweet (number of characters)\n",
        "train_data['tweet_length'] = train_data['text'].apply(len)\n",
        "\n",
        "# Plot the distribution of tweet lengths\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.histplot(train_data['tweet_length'], bins=20, kde=True)\n",
        "plt.title('Distribution of Tweet Lengths')\n",
        "plt.xlabel('Tweet Length (Number of Characters)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Step 3: Cleaning the text data\n",
        "\n",
        "# Function to clean the text by removing URLs, special characters, and hashtags\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
        "    text = re.sub(r\"#\\S+\", \"\", text)     # Remove hashtags\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove special characters and numbers\n",
        "    text = text.lower().strip()          # Convert to lowercase and remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Apply the cleaning function to the 'text' column in both train and test data\n",
        "train_data['cleaned_text'] = train_data['text'].apply(clean_text)\n",
        "test_data['cleaned_text'] = test_data['text'].apply(clean_text)\n",
        "\n",
        "# Display the cleaned text for the first few rows in the train dataset\n",
        "train_data[['text', 'cleaned_text']].head()\n",
        "\n",
        "# Display cleaned text\n",
        "print(train_data[['text', 'cleaned_text']].head())\n",
        "\n",
        "# Display the missing values in the train and test datasets\n",
        "missing_train, missing_test\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1C-CNrNyc1km",
        "outputId": "4bccac30-0d08-4ef1-dbaf-0a9c6d03b5e5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0  Our Deeds are the Reason of this #earthquake M...   \n",
            "1             Forest fire near La Ronge Sask. Canada   \n",
            "2  All residents asked to 'shelter in place' are ...   \n",
            "3  13,000 people receive #wildfires evacuation or...   \n",
            "4  Just got sent this photo from Ruby #Alaska as ...   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0  our deeds are the reason of this  may allah fo...  \n",
            "1              forest fire near la ronge sask canada  \n",
            "2  all residents asked to shelter in place are be...  \n",
            "3    people receive  evacuation orders in california  \n",
            "4  just got sent this photo from ruby  as smoke f...  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(id             0\n",
              " keyword       61\n",
              " location    2533\n",
              " text           0\n",
              " target         0\n",
              " dtype: int64,\n",
              " id             0\n",
              " keyword       26\n",
              " location    1105\n",
              " text           0\n",
              " dtype: int64)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1 Explaining the Visualizations and Data**\n",
        "\n",
        "#### **2.1.1 Distribution of Target Variable (Disaster vs Non-Disaster Tweets)**\n",
        "The initial graph displays the spread of the variable indicating whether a tweet discusses a disaster or not.\n",
        "- 0: Represents tweets that are not about real disasters.\n",
        "- 1: Represents tweets that are about real disasters.\n",
        "\n",
        "It appears that there are a number of non-disaster tweets (labeled 0) compared to disaster tweets (labeled 1). It might be necessary to handle this imbalance during model training to prevent bias, towards the class (non disaster).\n",
        "\n",
        "#### **2.1.1 The lengths of tweets vary widely.**\n",
        "The second graph shows how long the tweets are, in terms of character count giving an idea of the length of tweets.\n",
        "- Most tweets typically range from 100, to 140 characters in length. Often peak at, around the 140 character mark.\n",
        "- It's not surprising considering that tweets are limited, to 280 characters. Shorter tweets tend to be more common, in the dataset.\n",
        "- Within this range of 20 to 60 characters are some tweets as well.\n",
        "\n",
        "It's crucial to consider the range of tweet lengths as it plays a role, in making choices regarding text analysis and model selection such as deciding on the approach for longer tweets, in training sessions.\n",
        "\n",
        "\n",
        "### **2.2. Cleaning Text Data**\n",
        "I'm able to tidy up the text data by getting rid of characters, like URLs or hashtags and special symbols before classifying the tweets."
      ],
      "metadata": {
        "id": "qu70kRKJfBT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Model Architecture (25 pts)**\n",
        "I plan to utilize transformer based models such, as BERT and DistilBERT, for this task of classifying tweets into two categories; those related to a disaster and those that are not.\n",
        "\n",
        "### **3.1 Model 1: DistilBERT.**\n",
        "DistilBERT is, like a slimmed down BERT model â€“ it maintains, around 97 percent of BERTs capabilities while using six layers instead of twelve.\n",
        "It operates at a pace and, with better memory usage thanks, to knowledge distillation.\n",
        "#### What makes DistilBERT special and worth using in this context?\n",
        "It operates at a speed that's faster and has a size that's smaller compared to BERTs dimensions; this makes it suitable, for settings, with constrained computational capacities or when quicker deductions are necessary.\n",
        "\n",
        "\n",
        "### **3.2 Model 2: BERT**\n",
        "BERT is a trained transformer model that utilizes a bidirectional attention mechanism to comprehend the meaning of words by considering both the words that come before and, after, in a sentence.\n",
        "#### Why Choose BERT?\n",
        "BERTs bidirectional design allows it to grasp context, from both directions effectively to comprehend vague tweets accurately.\n",
        "Pre existing knowledge base;\" BERTs training results in an understanding of language nuances that can be effectively utilized for refining its performance in this particular assignment.\"\n",
        "\n",
        "### **3.3 In summary**\n",
        "BERT and DistilBERT are both suitable, for this task. BERT excels in performance, with its architecture while DistilBERT offers an inference option that is more resource efficient."
      ],
      "metadata": {
        "id": "8UUXqtYvlI-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, BertTokenizer, BertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Set up logging for progress tracking\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Step 2: Tokenization\n",
        "logger.info(\"Loading DistilBERT tokenizer...\")\n",
        "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "logger.info(\"Tokenizing training data for DistilBERT...\")\n",
        "train_texts = train_data['cleaned_text'].tolist()\n",
        "train_labels = train_data['target'].tolist()\n",
        "train_encodings = distilbert_tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
        "\n",
        "# Step 3: Convert data into PyTorch format\n",
        "logger.info(\"Converting data into PyTorch Dataset format...\")\n",
        "class DisasterDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = DisasterDataset(train_encodings, train_labels)\n",
        "\n",
        "# Step 4: Model Setup for DistilBERT\n",
        "logger.info(\"Initializing DistilBERT model for sequence classification...\")\n",
        "distilbert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
        "\n",
        "# Step 5: Training DistilBERT with detailed progress logging\n",
        "logger.info(\"Setting up training arguments and starting DistilBERT training...\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size for training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=100,               # log progress every 100 steps\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=distilbert_model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                             # training arguments, defined above\n",
        "    train_dataset=train_dataset                     # training dataset\n",
        ")\n",
        "\n",
        "logger.info(\"Training DistilBERT model...\")\n",
        "trainer.train()\n",
        "\n",
        "# Step 6: Predict using the DistilBERT model\n",
        "logger.info(\"Tokenizing test data for DistilBERT predictions...\")\n",
        "test_texts = test_data['cleaned_text'].tolist()\n",
        "\n",
        "# Tokenize test data\n",
        "test_encodings = distilbert_tokenizer(test_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "# Convert test data into PyTorch dataset\n",
        "logger.info(\"Converting test data into PyTorch Dataset format...\")\n",
        "class TestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "test_dataset = TestDataset(test_encodings)\n",
        "\n",
        "# Use the trained DistilBERT model to predict on test dataset\n",
        "logger.info(\"Making predictions with the trained DistilBERT model...\")\n",
        "\n",
        "# Perform prediction in batches using DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)  # Adjust batch size based on memory\n",
        "distilbert_model.eval()  # Ensure the model is in evaluation mode\n",
        "\n",
        "predictions = []\n",
        "\n",
        "# Move model to the same device as the data (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "distilbert_model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = distilbert_model(**batch)\n",
        "        logits = outputs.logits\n",
        "        batch_predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "        predictions.extend(batch_predictions)\n",
        "\n",
        "# Ensure the predictions length matches the test data length\n",
        "logger.info(f\"Number of predictions: {len(predictions)}\")\n",
        "logger.info(f\"Number of test samples: {len(test_data)}\")\n",
        "\n",
        "if len(predictions) != len(test_data):\n",
        "    logger.error(f\"Prediction length mismatch: {len(predictions)} predictions for {len(test_data)} test samples\")\n",
        "else:\n",
        "    # Step 7: Prepare the submission for DistilBERT predictions\n",
        "    logger.info(\"Saving DistilBERT predictions to CSV...\")\n",
        "    distilbert_submission = pd.DataFrame({\n",
        "        'id': test_data['id'],\n",
        "        'target': predictions\n",
        "    })\n",
        "    distilbert_submission.to_csv('drive/MyDrive/DTSA511/Week4/distilbert_submission.csv', index=False)\n",
        "    logger.info(\"DistilBERT predictions saved successfully.\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "wfn0fuQnOTei",
        "outputId": "a1d62ada-112b-4a93-98c6-5751af582215"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1428' max='1428' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1428/1428 00:44, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.656400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.481900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.412200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.446600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.442300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.362900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.386900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.372700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.347600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.291500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.223000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.242500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.206500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.216900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-05b26f514d30>:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Step 8: Repeat the same process for BERT\n",
        "logger.info(\"Loading BERT tokenizer...\")\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the training data for BERT\n",
        "logger.info(\"Tokenizing training data for BERT...\")\n",
        "train_encodings_bert = bert_tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
        "train_dataset_bert = DisasterDataset(train_encodings_bert, train_labels)\n",
        "\n",
        "# Initialize BERT model for sequence classification\n",
        "logger.info(\"Initializing BERT model for sequence classification...\")\n",
        "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Training BERT\n",
        "logger.info(\"Training BERT model...\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size for training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=100,               # log progress every 100 steps\n",
        ")\n",
        "\n",
        "trainer_bert = Trainer(\n",
        "    model=bert_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset_bert\n",
        ")\n",
        "\n",
        "trainer_bert.train()\n",
        "\n",
        "# Tokenize test data for BERT predictions\n",
        "logger.info(\"Tokenizing test data for BERT predictions...\")\n",
        "test_texts = test_data['cleaned_text'].tolist()\n",
        "test_encodings_bert = bert_tokenizer(test_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "# Convert test data into PyTorch dataset without labels\n",
        "class TestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: val[idx] for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "# Create dataset and DataLoader for batch processing\n",
        "logger.info(\"Converting test data into PyTorch Dataset format for BERT...\")\n",
        "test_dataset_bert = TestDataset(test_encodings_bert)\n",
        "test_loader_bert = DataLoader(test_dataset_bert, batch_size=16)\n",
        "\n",
        "# Ensure model is in evaluation mode\n",
        "bert_model.eval()\n",
        "\n",
        "# Move model to the correct device (GPU or CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "bert_model.to(device)\n",
        "\n",
        "# Making predictions in batches\n",
        "logger.info(\"Making predictions with the trained BERT model...\")\n",
        "bert_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader_bert:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = bert_model(**batch)\n",
        "        logits = outputs.logits\n",
        "        batch_predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "        bert_predictions.extend(batch_predictions)\n",
        "\n",
        "# Ensure the predictions length matches the test data length\n",
        "logger.info(f\"Number of predictions: {len(bert_predictions)}\")\n",
        "logger.info(f\"Number of test samples: {len(test_data)}\")\n",
        "\n",
        "if len(bert_predictions) != len(test_data):\n",
        "    logger.error(f\"Prediction length mismatch: {len(bert_predictions)} predictions for {len(test_data)} test samples\")\n",
        "else:\n",
        "    # Prepare submission for BERT predictions\n",
        "    logger.info(\"Saving BERT predictions to CSV...\")\n",
        "    bert_submission = pd.DataFrame({\n",
        "        'id': test_data['id'],\n",
        "        'target': bert_predictions\n",
        "    })\n",
        "    bert_submission.to_csv('drive/MyDrive/DTSA511/Week4/bert_submission.csv', index=False)\n",
        "    logger.info(\"BERT predictions saved successfully.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "JR5rQNWrOYIK",
        "outputId": "25572776-31d0-41fb-8f2f-9ce1deb9a5ee"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1428' max='1428' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1428/1428 01:16, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.629500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.468100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.412800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.461200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.451900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.370300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.398000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.372800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.375100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.325000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.239000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.266400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.227000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.228500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import EvalPrediction\n",
        "\n",
        "# Define the compute_metrics function\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    return {\"accuracy\": accuracy_score(p.label_ids, preds)}\n",
        "\n",
        "# Function to evaluate and store results\n",
        "def evaluate_model(model_name, train_dataset, val_dataset, learning_rate):\n",
        "    # Initialize model\n",
        "    if model_name == 'distilbert':\n",
        "        model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
        "    else:\n",
        "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Set up training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=3,  # Run for a few epochs only for tuning\n",
        "        evaluation_strategy=\"steps\",\n",
        "        logging_steps=100,\n",
        "        eval_steps=500,\n",
        "        save_steps=500,\n",
        "        load_best_model_at_end=True,\n",
        "        logging_dir='./logs',\n",
        "    )\n",
        "\n",
        "    # Initialize Trainer with the compute_metrics function\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics  # Pass compute_metrics to Trainer\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate the model and return accuracy\n",
        "    eval_results = trainer.evaluate()\n",
        "    return eval_results['eval_accuracy']\n",
        "\n",
        "# Now run the same loop for hyperparameter tuning\n",
        "learning_rates = [1e-4, 5e-5, 1e-5]\n",
        "results = {'learning_rate': [], 'accuracy_distilbert': [], 'accuracy_bert': []}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    # Train and evaluate DistilBERT\n",
        "    distilbert_accuracy = evaluate_model('distilbert', train_dataset, val_dataset, lr)\n",
        "\n",
        "    # Train and evaluate BERT\n",
        "    bert_accuracy = evaluate_model('bert', train_dataset, val_dataset, lr)\n",
        "\n",
        "    # Log the results\n",
        "    results['learning_rate'].append(lr)\n",
        "    results['accuracy_distilbert'].append(distilbert_accuracy)\n",
        "    results['accuracy_bert'].append(bert_accuracy)\n",
        "\n",
        "# Display results as a DataFrame for easier viewing\n",
        "import pandas as pd\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E5bCH92fwhJk",
        "outputId": "8e01875d-8601-4e41-f17b-2e92c9297852"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1287' max='1287' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1287/1287 00:42, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.361700</td>\n",
              "      <td>0.475131</td>\n",
              "      <td>0.817585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.214200</td>\n",
              "      <td>0.593530</td>\n",
              "      <td>0.797900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1287' max='1287' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1287/1287 01:08, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.391100</td>\n",
              "      <td>0.508351</td>\n",
              "      <td>0.804462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.256200</td>\n",
              "      <td>0.561141</td>\n",
              "      <td>0.821522</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1287' max='1287' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1287/1287 00:42, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.349900</td>\n",
              "      <td>0.465820</td>\n",
              "      <td>0.820210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.213200</td>\n",
              "      <td>0.546902</td>\n",
              "      <td>0.822835</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1287' max='1287' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1287/1287 01:09, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.365500</td>\n",
              "      <td>0.558420</td>\n",
              "      <td>0.790026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.223200</td>\n",
              "      <td>0.549396</td>\n",
              "      <td>0.825459</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1287' max='1287' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1287/1287 00:42, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.389700</td>\n",
              "      <td>0.433497</td>\n",
              "      <td>0.821522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.345700</td>\n",
              "      <td>0.423410</td>\n",
              "      <td>0.825459</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1287' max='1287' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1287/1287 01:08, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.379200</td>\n",
              "      <td>0.410305</td>\n",
              "      <td>0.837270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.322700</td>\n",
              "      <td>0.425642</td>\n",
              "      <td>0.832021</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   learning_rate  accuracy_distilbert  accuracy_bert\n",
            "0        0.00010             0.817585       0.804462\n",
            "1        0.00005             0.820210       0.825459\n",
            "2        0.00001             0.825459       0.837270\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Set up logging for progress tracking\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Step 2: Tokenization\n",
        "logger.info(\"Loading DistilBERT tokenizer...\")\n",
        "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "logger.info(\"Tokenizing training data for DistilBERT...\")\n",
        "train_texts = train_data['cleaned_text'].tolist()\n",
        "train_labels = train_data['target'].tolist()\n",
        "train_encodings = distilbert_tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
        "\n",
        "# Step 3: Convert data into PyTorch format\n",
        "logger.info(\"Converting data into PyTorch Dataset format...\")\n",
        "class DisasterDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = DisasterDataset(train_encodings, train_labels)\n",
        "\n",
        "# Step 4: Model Setup for DistilBERT\n",
        "logger.info(\"Initializing DistilBERT model for sequence classification...\")\n",
        "distilbert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
        "\n",
        "# Step 5: Training DistilBERT with optimal learning rate (1e-5)\n",
        "logger.info(\"Setting up training arguments and starting DistilBERT training...\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size for training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=100,               # log progress every 100 steps\n",
        "    learning_rate=1e-5,              # *** Set the optimal learning rate to 1e-5 ***\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=distilbert_model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                             # training arguments, defined above\n",
        "    train_dataset=train_dataset                     # training dataset\n",
        ")\n",
        "\n",
        "logger.info(\"Training DistilBERT model...\")\n",
        "trainer.train()\n",
        "\n",
        "# Step 6: Predict using the DistilBERT model\n",
        "logger.info(\"Tokenizing test data for DistilBERT predictions...\")\n",
        "test_texts = test_data['cleaned_text'].tolist()\n",
        "\n",
        "# Tokenize test data\n",
        "test_encodings = distilbert_tokenizer(test_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "# Convert test data into PyTorch dataset\n",
        "logger.info(\"Converting test data into PyTorch Dataset format...\")\n",
        "class TestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "test_dataset = TestDataset(test_encodings)\n",
        "\n",
        "# Use the trained DistilBERT model to predict on test dataset\n",
        "logger.info(\"Making predictions with the trained DistilBERT model...\")\n",
        "\n",
        "# Perform prediction in batches using DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)  # Adjust batch size based on memory\n",
        "distilbert_model.eval()  # Ensure the model is in evaluation mode\n",
        "\n",
        "predictions = []\n",
        "\n",
        "# Move model to the same device as the data (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "distilbert_model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = distilbert_model(**batch)\n",
        "        logits = outputs.logits\n",
        "        batch_predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "        predictions.extend(batch_predictions)\n",
        "\n",
        "# Ensure the predictions length matches the test data length\n",
        "logger.info(f\"Number of predictions: {len(predictions)}\")\n",
        "logger.info(f\"Number of test samples: {len(test_data)}\")\n",
        "\n",
        "if len(predictions) != len(test_data):\n",
        "    logger.error(f\"Prediction length mismatch: {len(predictions)} predictions for {len(test_data)} test samples\")\n",
        "else:\n",
        "    # Step 7: Prepare the submission for DistilBERT predictions\n",
        "    logger.info(\"Saving DistilBERT predictions to CSV...\")\n",
        "    distilbert_submission = pd.DataFrame({\n",
        "        'id': test_data['id'],\n",
        "        'target': predictions\n",
        "    })\n",
        "    distilbert_submission.to_csv('drive/MyDrive/DTSA511/Week4/distilbert_submission.csv', index=False)\n",
        "    logger.info(\"DistilBERT predictions saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "dP7zWIoR2RXJ",
        "outputId": "a8a41cdd-38ec-4ac2-e9ac-2b3d57298ecf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1428' max='1428' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1428/1428 00:45, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.686900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.624000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.491000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.462900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.442000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.399300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.416500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.396800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.376900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.357800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.331400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.368500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.333200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.326800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-d72d45ce9097>:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Step 8: Repeat the same process for BERT\n",
        "logger.info(\"Loading BERT tokenizer...\")\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the training data for BERT\n",
        "logger.info(\"Tokenizing training data for BERT...\")\n",
        "train_encodings_bert = bert_tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
        "train_dataset_bert = DisasterDataset(train_encodings_bert, train_labels)\n",
        "\n",
        "# Initialize BERT model for sequence classification\n",
        "logger.info(\"Initializing BERT model for sequence classification...\")\n",
        "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Training BERT with optimal learning rate (1e-5)\n",
        "logger.info(\"Training BERT model...\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size for training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=100,               # log progress every 100 steps\n",
        "    learning_rate=1e-5               # *** Set the optimal learning rate to 1e-5 ***\n",
        ")\n",
        "\n",
        "trainer_bert = Trainer(\n",
        "    model=bert_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset_bert\n",
        ")\n",
        "\n",
        "trainer_bert.train()\n",
        "\n",
        "# Tokenize test data for BERT predictions\n",
        "logger.info(\"Tokenizing test data for BERT predictions...\")\n",
        "test_texts = test_data['cleaned_text'].tolist()\n",
        "test_encodings_bert = bert_tokenizer(test_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "# Convert test data into PyTorch dataset without labels\n",
        "class TestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: val[idx] for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "# Create dataset and DataLoader for batch processing\n",
        "logger.info(\"Converting test data into PyTorch Dataset format for BERT...\")\n",
        "test_dataset_bert = TestDataset(test_encodings_bert)\n",
        "test_loader_bert = DataLoader(test_dataset_bert, batch_size=16)\n",
        "\n",
        "# Ensure model is in evaluation mode\n",
        "bert_model.eval()\n",
        "\n",
        "# Move model to the correct device (GPU or CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "bert_model.to(device)\n",
        "\n",
        "# Making predictions in batches\n",
        "logger.info(\"Making predictions with the trained BERT model...\")\n",
        "bert_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader_bert:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = bert_model(**batch)\n",
        "        logits = outputs.logits\n",
        "        batch_predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "        bert_predictions.extend(batch_predictions)\n",
        "\n",
        "# Ensure the predictions length matches the test data length\n",
        "logger.info(f\"Number of predictions: {len(bert_predictions)}\")\n",
        "logger.info(f\"Number of test samples: {len(test_data)}\")\n",
        "\n",
        "if len(bert_predictions) != len(test_data):\n",
        "    logger.error(f\"Prediction length mismatch: {len(bert_predictions)} predictions for {len(test_data)} test samples\")\n",
        "else:\n",
        "    # Prepare submission for BERT predictions\n",
        "    logger.info(\"Saving BERT predictions to CSV...\")\n",
        "    bert_submission = pd.DataFrame({\n",
        "        'id': test_data['id'],\n",
        "        'target': bert_predictions\n",
        "    })\n",
        "    bert_submission.to_csv('drive/MyDrive/DTSA511/Week4/bert_submission.csv', index=False)\n",
        "    logger.info(\"BERT predictions saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "EgxTz7HR3c2E",
        "outputId": "cc94b9f0-ba36-4ace-fa3c-ffc84ac1eeae"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1428' max='1428' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1428/1428 01:14, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.682300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.571700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.493000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.470300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.447500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.388900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.407500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.392000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.374500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.352800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.317500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.352300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.313100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.306400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Results and Analysis (35 pts)**\n",
        "\n",
        "### **4.1 learning rates in performance**\n",
        "I have the results from my learning rate tuning experiments for both DistilBERT and BERT. Here's how I can interpret and present the results.\n",
        "\n",
        "**Table: Impact of Learning Rate Tuning**\n",
        "\n",
        "| Learning Rate | Accuracy (DistilBERT) | Accuracy (BERT) |\n",
        "|---------------|-----------------------|-----------------|\n",
        "| 1e-4          | 0.8176                | 0.8045          |\n",
        "| 5e-5          | 0.8202                | 0.8255          |\n",
        "| 1e-5          | 0.8255                | 0.8373          |\n",
        "\n",
        "\n",
        "### **4.2 Understanding the effectiveness of learning rates in performance**\n",
        "\n",
        "- DistilBERT performed best with an accuracy of 0.8255 when using a learning rate of 1e-5.\n",
        "- BERT excelled with an accuracy of 0.8373 when the learning rate was set to 1e-5.\n",
        "\n",
        "### **4.3 DistilBERT, versus BERT**\n",
        "\n",
        "- BERT generally had accuracy, with learning rates than DistilBERT did well with a smaller model size and quicker training times.\n",
        "- When it comes to efficiency as a priority factor to consider between - DistilBERT and BERT models DistilBERT might be the option; however BERT tends to provide slightly better accuracy, in comparison.\n",
        "- In the case of DistilBERT and BERT models it seems that a learning rate of 1e-5 is effective since it led to the accuracy, for both models.\n",
        "\n",
        "### **4.4 The result of Kaggle Scores**\n",
        "I set learning_rate=1e-5 and trained the DistilBERT model and BERT model, and summarized the Kaggle scores for the prediction values in the table below. As shown in the table below, the BERT model scored better than the DistilBERT model.\n",
        "\n",
        "|             | DistilBERT | BERT    |\n",
        "|-------------|-------------|---------|\n",
        "| Kaggle Score| 0.82623     | 0.82960 |\n",
        "\n"
      ],
      "metadata": {
        "id": "TKolizvalv8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Conclusion (15 pts)**\n",
        "My project centered on using transformer based models, like DistilBERT and BERT for categorizing tweets related to disasters effectively. I tested learning rates. Assessed how well each model performed to find the best setup, for making precise predictions.\n",
        "\n",
        "### **5.1 Key Learnings and Results**\n",
        "#### **5.1.1 Model Performance**\n",
        "- **DistilBERT** is a quicker version of BERT that delivers solid performance by achieving an accuracy of 0.8255 using a learning rate of 1e-5. Its ability to train efficiently and consume memory makes it an excellent choice, for situations where computing power is constrained.\n",
        "\n",
        "- The initial **BERT** model performed better, than DistilBERT with an increased accuracy of 0.8373 using the learning rate of 1e-5. However BERT demands computing power. Requires a longer training period.\n",
        "\n",
        "#### **5.1.2 Learning Rate Tuning**\n",
        "- I experimented with three learning rates: 1e-4, 5e-5, and 1e-5. Through my experiments, I found that 1e-5 was the optimal learning rate for both models, yielding the highest accuracy.\n",
        "- Higher learning rates (e.g., 1e-4) resulted in lower performance, likely due to overshooting the optimal point during training.\n",
        "\n",
        "### **5.2.What Worked Well**\n",
        "- Transformer Models, like DistilBERT and BERT showed results in categorizing tweets related to disasters due to their attention mechanisms that effectively grasp the context and subtleties in the text, for forecasts.\n",
        "- Fine tuning the learning rate played a role in enhancing the models performance, with the learning rate set at 1e-5 leading to a notable increase in accuracy, for both models.\n",
        "- Despite being a model, in comparison to others in its class DistilBERT delivers performance at a quicker pace during training making it a great option when there are limitations, in resources or time available.\n",
        "\n",
        "### **5.3 Challenges and What Didn't Work**\n",
        "- BERTs resource needs were higher even though it provided accuracy, than other methods.These increased demands, on memory and training time could potentially slow down processes when dealing with datasets or more intricate tasks.\n",
        "- In some instances, during my runs of the model training process I applied stopping; however sometimes it ended the training phase soon. To avoid this problem in iterations I may need to do some fine tuning or make changes, to the patience value.\n",
        "\n",
        "\n",
        "### **5.4 Future Improvements**\n",
        "- Fine tuning hyperparameters is crucial, in enhancing performance beyond adjusting the learning rate; factors such as batch size and the duration of warmup steps should also be considered for optimization, over epochs.\n",
        "- Increasing the duration of training sessions could help the models achieve convergence outcomes; this effect may be enhanced by reducing the rate of learning decay.\n",
        "- Exploring tokenization methods and utilizing trained models customized for specific domains can improve the handling of disaster related language effectively.\n",
        "\n",
        "### **5.5 Final Takeaway**\n",
        "Both DistilBERT and BERT have shown effectiveness, in categorizing tweets about disasters accurately and efficiently with characteristics; DistilBERT striking a balance between speed and precision, for real world scenarios; conversely BERT outperforming slightly but requiring longer training periods and more memory usage.\n",
        "\n",
        "In future work, diving into more sophisticated methods, like fine tuning additional settings may lead to enhanced outcomes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oXT3nbCt8O1B"
      }
    }
  ]
}